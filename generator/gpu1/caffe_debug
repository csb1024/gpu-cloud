I1018 03:59:56.159013  1063 caffe.cpp:279] Using GPUs 1
I1018 03:59:56.317412  1063 caffe.cpp:284] GPU 1: GeForce GTX 1080
I1018 03:59:56.921744  1063 solver.cpp:44] Initializing solver from parameters: 
test_iter: 1
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 0
solver_mode: GPU
device_id: 1
net: "/home/sbchoi/git/gpu-cloud/generator/gpu1/out.prototxt"
train_state {
  level: 0
  stage: ""
}
I1018 03:59:56.921825  1063 solver.cpp:87] Creating training net from net file: /home/sbchoi/git/gpu-cloud/generator/gpu1/out.prototxt
I1018 03:59:56.922222  1063 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer x_test
I1018 03:59:56.922236  1063 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer y_test
I1018 03:59:56.922246  1063 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I1018 03:59:56.922329  1063 net.cpp:53] Initializing net from parameters: 
name: "DummyNetbyHand"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "x_train"
  type: "DummyData"
  top: "data"
  include {
    phase: TRAIN
  }
  dummy_data_param {
    data_filler {
      type: "constant"
    }
    shape {
      dim: 128
      dim: 23
      dim: 53
      dim: 53
    }
  }
}
layer {
  name: "y_train"
  type: "DummyData"
  top: "label"
  include {
    phase: TRAIN
  }
  dummy_data_param {
    data_filler {
      type: "constant"
    }
    shape {
      dim: 128
    }
  }
}
layer {
  name: "IP"
  type: "InnerProduct"
  bottom: "data"
  top: "ip2"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 35
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "RELU"
  type: "ReLU"
  bottom: "ip2"
  top: "relu1"
}
layer {
  name: "IP2"
  type: "InnerProduct"
  bottom: "relu1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 1
  }
  inner_product_param {
    num_output: 4080
    weight_filler {
      type: "gaussian"
    }
    bias_filler {
      type: "gaussian"
    }
  }
}
layer {
  name: "SOFTMAX"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1018 03:59:56.922411  1063 layer_factory.hpp:77] Creating layer x_train
I1018 03:59:56.922436  1063 net.cpp:86] Creating Layer x_train
I1018 03:59:56.922448  1063 net.cpp:382] x_train -> data
I1018 03:59:56.950218  1063 net.cpp:124] Setting up x_train
I1018 03:59:56.950295  1063 net.cpp:131] Top shape: 128 23 53 53 (8269696)
I1018 03:59:56.950302  1063 net.cpp:139] Memory required for data: 33078784
I1018 03:59:56.950318  1063 layer_factory.hpp:77] Creating layer y_train
I1018 03:59:56.950353  1063 net.cpp:86] Creating Layer y_train
I1018 03:59:56.950366  1063 net.cpp:382] y_train -> label
I1018 03:59:56.950472  1063 net.cpp:124] Setting up y_train
I1018 03:59:56.950487  1063 net.cpp:131] Top shape: 128 (128)
I1018 03:59:56.950493  1063 net.cpp:139] Memory required for data: 33079296
I1018 03:59:56.950500  1063 layer_factory.hpp:77] Creating layer IP
I1018 03:59:56.950515  1063 net.cpp:86] Creating Layer IP
I1018 03:59:56.950523  1063 net.cpp:408] IP <- data
I1018 03:59:56.950538  1063 net.cpp:382] IP -> ip2
I1018 03:59:57.030220  1063 net.cpp:124] Setting up IP
I1018 03:59:57.030242  1063 net.cpp:131] Top shape: 128 35 (4480)
I1018 03:59:57.030248  1063 net.cpp:139] Memory required for data: 33097216
I1018 03:59:57.030270  1063 layer_factory.hpp:77] Creating layer RELU
I1018 03:59:57.030285  1063 net.cpp:86] Creating Layer RELU
I1018 03:59:57.030292  1063 net.cpp:408] RELU <- ip2
I1018 03:59:57.030299  1063 net.cpp:382] RELU -> relu1
I1018 03:59:57.415391  1063 net.cpp:124] Setting up RELU
I1018 03:59:57.415446  1063 net.cpp:131] Top shape: 128 35 (4480)
I1018 03:59:57.415453  1063 net.cpp:139] Memory required for data: 33115136
I1018 03:59:57.415463  1063 layer_factory.hpp:77] Creating layer IP2
I1018 03:59:57.415504  1063 net.cpp:86] Creating Layer IP2
I1018 03:59:57.415524  1063 net.cpp:408] IP2 <- relu1
I1018 03:59:57.415537  1063 net.cpp:382] IP2 -> ip1
I1018 03:59:57.423329  1063 net.cpp:124] Setting up IP2
I1018 03:59:57.423352  1063 net.cpp:131] Top shape: 128 4080 (522240)
I1018 03:59:57.423357  1063 net.cpp:139] Memory required for data: 35204096
I1018 03:59:57.423372  1063 layer_factory.hpp:77] Creating layer SOFTMAX
I1018 03:59:57.423393  1063 net.cpp:86] Creating Layer SOFTMAX
I1018 03:59:57.423399  1063 net.cpp:408] SOFTMAX <- ip1
I1018 03:59:57.423406  1063 net.cpp:408] SOFTMAX <- label
I1018 03:59:57.423415  1063 net.cpp:382] SOFTMAX -> loss
I1018 03:59:57.423437  1063 layer_factory.hpp:77] Creating layer SOFTMAX
I1018 03:59:57.425885  1063 net.cpp:124] Setting up SOFTMAX
I1018 03:59:57.425906  1063 net.cpp:131] Top shape: (1)
I1018 03:59:57.425912  1063 net.cpp:134]     with loss weight 1
I1018 03:59:57.425935  1063 net.cpp:139] Memory required for data: 35204100
I1018 03:59:57.425941  1063 net.cpp:200] SOFTMAX needs backward computation.
I1018 03:59:57.425948  1063 net.cpp:200] IP2 needs backward computation.
I1018 03:59:57.425953  1063 net.cpp:200] RELU needs backward computation.
I1018 03:59:57.425958  1063 net.cpp:200] IP needs backward computation.
I1018 03:59:57.425964  1063 net.cpp:202] y_train does not need backward computation.
I1018 03:59:57.425969  1063 net.cpp:202] x_train does not need backward computation.
I1018 03:59:57.425974  1063 net.cpp:244] This network produces output loss
I1018 03:59:57.425987  1063 net.cpp:257] Network initialization done.
I1018 03:59:57.426352  1063 solver.cpp:173] Creating test net (#0) specified by net file: /home/sbchoi/git/gpu-cloud/generator/gpu1/out.prototxt
I1018 03:59:57.426381  1063 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer x_train
I1018 03:59:57.426389  1063 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer y_train
I1018 03:59:57.426473  1063 net.cpp:53] Initializing net from parameters: 
name: "DummyNetbyHand"
state {
  phase: TEST
}
layer {
  name: "x_test"
  type: "DummyData"
  top: "data"
  include {
    phase: TEST
  }
  dummy_data_param {
    data_filler {
      type: "constant"
    }
    shape {
      dim: 128
      dim: 23
      dim: 53
      dim: 53
    }
  }
}
layer {
  name: "y_test"
  type: "DummyData"
  top: "label"
  include {
    phase: TEST
  }
  dummy_data_param {
    data_filler {
      type: "constant"
    }
    shape {
      dim: 128
    }
  }
}
layer {
  name: "IP"
  type: "InnerProduct"
  bottom: "data"
  top: "ip2"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 35
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "RELU"
  type: "ReLU"
  bottom: "ip2"
  top: "relu1"
}
layer {
  name: "IP2"
  type: "InnerProduct"
  bottom: "relu1"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 1
  }
  inner_product_param {
    num_output: 4080
    weight_filler {
      type: "gaussian"
    }
    bias_filler {
      type: "gaussian"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip1"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "SOFTMAX"
  type: "SoftmaxWithLoss"
  bottom: "ip1"
  bottom: "label"
  top: "loss"
}
I1018 03:59:57.426547  1063 layer_factory.hpp:77] Creating layer x_test
I1018 03:59:57.426560  1063 net.cpp:86] Creating Layer x_test
I1018 03:59:57.426568  1063 net.cpp:382] x_test -> data
I1018 03:59:57.452409  1063 net.cpp:124] Setting up x_test
I1018 03:59:57.452466  1063 net.cpp:131] Top shape: 128 23 53 53 (8269696)
I1018 03:59:57.452474  1063 net.cpp:139] Memory required for data: 33078784
I1018 03:59:57.452484  1063 layer_factory.hpp:77] Creating layer y_test
I1018 03:59:57.452508  1063 net.cpp:86] Creating Layer y_test
I1018 03:59:57.452517  1063 net.cpp:382] y_test -> label
I1018 03:59:57.452651  1063 net.cpp:124] Setting up y_test
I1018 03:59:57.452677  1063 net.cpp:131] Top shape: 128 (128)
I1018 03:59:57.452682  1063 net.cpp:139] Memory required for data: 33079296
I1018 03:59:57.452688  1063 layer_factory.hpp:77] Creating layer label_y_test_0_split
I1018 03:59:57.452699  1063 net.cpp:86] Creating Layer label_y_test_0_split
I1018 03:59:57.452705  1063 net.cpp:408] label_y_test_0_split <- label
I1018 03:59:57.452715  1063 net.cpp:382] label_y_test_0_split -> label_y_test_0_split_0
I1018 03:59:57.452725  1063 net.cpp:382] label_y_test_0_split -> label_y_test_0_split_1
I1018 03:59:57.452762  1063 net.cpp:124] Setting up label_y_test_0_split
I1018 03:59:57.452770  1063 net.cpp:131] Top shape: 128 (128)
I1018 03:59:57.452776  1063 net.cpp:131] Top shape: 128 (128)
I1018 03:59:57.452781  1063 net.cpp:139] Memory required for data: 33080320
I1018 03:59:57.452786  1063 layer_factory.hpp:77] Creating layer IP
I1018 03:59:57.452798  1063 net.cpp:86] Creating Layer IP
I1018 03:59:57.452805  1063 net.cpp:408] IP <- data
I1018 03:59:57.452813  1063 net.cpp:382] IP -> ip2
I1018 03:59:57.532088  1063 net.cpp:124] Setting up IP
I1018 03:59:57.532109  1063 net.cpp:131] Top shape: 128 35 (4480)
I1018 03:59:57.532116  1063 net.cpp:139] Memory required for data: 33098240
I1018 03:59:57.532131  1063 layer_factory.hpp:77] Creating layer RELU
I1018 03:59:57.532143  1063 net.cpp:86] Creating Layer RELU
I1018 03:59:57.532150  1063 net.cpp:408] RELU <- ip2
I1018 03:59:57.532160  1063 net.cpp:382] RELU -> relu1
I1018 03:59:57.532459  1063 net.cpp:124] Setting up RELU
I1018 03:59:57.532472  1063 net.cpp:131] Top shape: 128 35 (4480)
I1018 03:59:57.532479  1063 net.cpp:139] Memory required for data: 33116160
I1018 03:59:57.532485  1063 layer_factory.hpp:77] Creating layer IP2
I1018 03:59:57.532496  1063 net.cpp:86] Creating Layer IP2
I1018 03:59:57.532502  1063 net.cpp:408] IP2 <- relu1
I1018 03:59:57.532512  1063 net.cpp:382] IP2 -> ip1
I1018 03:59:57.537544  1063 net.cpp:124] Setting up IP2
I1018 03:59:57.537557  1063 net.cpp:131] Top shape: 128 4080 (522240)
I1018 03:59:57.537562  1063 net.cpp:139] Memory required for data: 35205120
I1018 03:59:57.537573  1063 layer_factory.hpp:77] Creating layer ip1_IP2_0_split
I1018 03:59:57.537583  1063 net.cpp:86] Creating Layer ip1_IP2_0_split
I1018 03:59:57.537590  1063 net.cpp:408] ip1_IP2_0_split <- ip1
I1018 03:59:57.537598  1063 net.cpp:382] ip1_IP2_0_split -> ip1_IP2_0_split_0
I1018 03:59:57.537607  1063 net.cpp:382] ip1_IP2_0_split -> ip1_IP2_0_split_1
I1018 03:59:57.537645  1063 net.cpp:124] Setting up ip1_IP2_0_split
I1018 03:59:57.537653  1063 net.cpp:131] Top shape: 128 4080 (522240)
I1018 03:59:57.537660  1063 net.cpp:131] Top shape: 128 4080 (522240)
I1018 03:59:57.537664  1063 net.cpp:139] Memory required for data: 39383040
I1018 03:59:57.537670  1063 layer_factory.hpp:77] Creating layer accuracy
I1018 03:59:57.537688  1063 net.cpp:86] Creating Layer accuracy
I1018 03:59:57.537693  1063 net.cpp:408] accuracy <- ip1_IP2_0_split_0
I1018 03:59:57.537700  1063 net.cpp:408] accuracy <- label_y_test_0_split_0
I1018 03:59:57.537708  1063 net.cpp:382] accuracy -> accuracy
I1018 03:59:57.537719  1063 net.cpp:124] Setting up accuracy
I1018 03:59:57.537726  1063 net.cpp:131] Top shape: (1)
I1018 03:59:57.537731  1063 net.cpp:139] Memory required for data: 39383044
I1018 03:59:57.537736  1063 layer_factory.hpp:77] Creating layer SOFTMAX
I1018 03:59:57.537746  1063 net.cpp:86] Creating Layer SOFTMAX
I1018 03:59:57.537751  1063 net.cpp:408] SOFTMAX <- ip1_IP2_0_split_1
I1018 03:59:57.537758  1063 net.cpp:408] SOFTMAX <- label_y_test_0_split_1
I1018 03:59:57.537765  1063 net.cpp:382] SOFTMAX -> loss
I1018 03:59:57.537776  1063 layer_factory.hpp:77] Creating layer SOFTMAX
I1018 03:59:57.540004  1063 net.cpp:124] Setting up SOFTMAX
I1018 03:59:57.540022  1063 net.cpp:131] Top shape: (1)
I1018 03:59:57.540030  1063 net.cpp:134]     with loss weight 1
I1018 03:59:57.540045  1063 net.cpp:139] Memory required for data: 39383048
I1018 03:59:57.540050  1063 net.cpp:200] SOFTMAX needs backward computation.
I1018 03:59:57.540065  1063 net.cpp:202] accuracy does not need backward computation.
I1018 03:59:57.540081  1063 net.cpp:200] ip1_IP2_0_split needs backward computation.
I1018 03:59:57.540086  1063 net.cpp:200] IP2 needs backward computation.
I1018 03:59:57.540091  1063 net.cpp:200] RELU needs backward computation.
I1018 03:59:57.540097  1063 net.cpp:200] IP needs backward computation.
I1018 03:59:57.540103  1063 net.cpp:202] label_y_test_0_split does not need backward computation.
I1018 03:59:57.540109  1063 net.cpp:202] y_test does not need backward computation.
I1018 03:59:57.540114  1063 net.cpp:202] x_test does not need backward computation.
I1018 03:59:57.540119  1063 net.cpp:244] This network produces output accuracy
I1018 03:59:57.540125  1063 net.cpp:244] This network produces output loss
I1018 03:59:57.540139  1063 net.cpp:257] Network initialization done.
I1018 03:59:57.540194  1063 solver.cpp:56] Solver scaffolding done.
I1018 03:59:57.540379  1063 caffe.cpp:309] Starting Optimization
I1018 03:59:57.540387  1063 solver.cpp:288] Solving DummyNetbyHand
I1018 03:59:57.540392  1063 solver.cpp:289] Learning Rate Policy: inv
I1018 03:59:57.541308  1063 solver.cpp:346] Iteration 0, Testing net (#0)
I1018 03:59:57.541327  1063 net.cpp:687] Ignoring source layer x_train
I1018 03:59:57.541333  1063 net.cpp:687] Ignoring source layer y_train
I1018 03:59:57.543076  1063 net.cpp:596]  [Forward] [x_test] top blob data data size: 33078784
I1018 03:59:57.543092  1063 net.cpp:596]  [Forward] [y_test] top blob label data size: 512
I1018 03:59:57.543104  1063 net.cpp:596]  [Forward] [label_y_test_0_split] top blob label_y_test_0_split_0 data size: 512
I1018 03:59:57.543109  1063 net.cpp:596]  [Forward] [label_y_test_0_split] top blob label_y_test_0_split_1 data size: 512
I1018 03:59:57.552423  1063 net.cpp:596]  [Forward] [IP] top blob ip2 data size: 17920
I1018 03:59:57.552443  1063 net.cpp:610]  [Forward]  [IP] param blob 0 data size: 9044980
I1018 03:59:57.552448  1063 net.cpp:610]  [Forward]  [IP] param blob 1 data size: 140
I1018 03:59:57.562978  1063 net.cpp:596]  [Forward] [RELU] top blob relu1 data size: 17920
I1018 03:59:57.563647  1063 net.cpp:596]  [Forward] [IP2] top blob ip1 data size: 2088960
I1018 03:59:57.563664  1063 net.cpp:610]  [Forward]  [IP2] param blob 0 data size: 571200
I1018 03:59:57.563669  1063 net.cpp:610]  [Forward]  [IP2] param blob 1 data size: 16320
I1018 03:59:57.563678  1063 net.cpp:596]  [Forward] [ip1_IP2_0_split] top blob ip1_IP2_0_split_0 data size: 2088960
I1018 03:59:57.563683  1063 net.cpp:596]  [Forward] [ip1_IP2_0_split] top blob ip1_IP2_0_split_1 data size: 2088960
I1018 03:59:57.573444  1063 net.cpp:596]  [Forward] [accuracy] top blob accuracy data size: 4
I1018 03:59:57.575121  1063 net.cpp:596]  [Forward] [SOFTMAX] top blob loss data size: 4
I1018 03:59:57.575142  1063 solver.cpp:414]     Test net output #0: accuracy = 0
I1018 03:59:57.575160  1063 solver.cpp:414]     Test net output #1: loss = 9.14717 (* 1 = 9.14717 loss)
I1018 03:59:57.575173  1063 net.cpp:596]  [Forward] [x_train] top blob data data size: 33078784
I1018 03:59:57.575179  1063 net.cpp:596]  [Forward] [y_train] top blob label data size: 512
I1018 03:59:57.583200  1063 net.cpp:596]  [Forward] [IP] top blob ip2 data size: 17920
I1018 03:59:57.583217  1063 net.cpp:610]  [Forward]  [IP] param blob 0 data size: 9044980
I1018 03:59:57.583223  1063 net.cpp:610]  [Forward]  [IP] param blob 1 data size: 140
I1018 03:59:57.589751  1063 net.cpp:596]  [Forward] [RELU] top blob relu1 data size: 17920
I1018 03:59:57.590250  1063 net.cpp:596]  [Forward] [IP2] top blob ip1 data size: 2088960
I1018 03:59:57.590266  1063 net.cpp:610]  [Forward]  [IP2] param blob 0 data size: 571200
I1018 03:59:57.590272  1063 net.cpp:610]  [Forward]  [IP2] param blob 1 data size: 16320
I1018 03:59:57.591773  1063 net.cpp:596]  [Forward] [SOFTMAX] top blob loss data size: 4
I1018 03:59:57.591913  1063 net.cpp:628]  [Backward] [SOFTMAX] bottom blob ip1 diff size: 2088960
I1018 03:59:57.592126  1063 net.cpp:628]  [Backward] [IP2] bottom blob relu1 diff size: 17920
I1018 03:59:57.592144  1063 net.cpp:641]  [Backward] [IP2] param blob 0 diff size: 571200
I1018 03:59:57.592159  1063 net.cpp:641]  [Backward] [IP2] param blob 1 diff size: 16320
I1018 03:59:57.592219  1063 net.cpp:628]  [Backward] [RELU] bottom blob ip2 diff size: 17920
I1018 03:59:57.592319  1063 net.cpp:641]  [Backward] [IP] param blob 0 diff size: 9044980
I1018 03:59:57.592330  1063 net.cpp:641]  [Backward] [IP] param blob 1 diff size: 140
I1018 03:59:57.592950  1063 solver.cpp:221] Iteration 0 (0 iter/s, 0.05251s/100 iters), loss = 9.14717
I1018 03:59:57.592975  1063 solver.cpp:240]     Train net output #0: loss = 9.14717 (* 1 = 9.14717 loss)
I1018 03:59:57.592991  1063 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I1018 03:59:57.593659  1063 solver.cpp:464] Snapshotting to binary proto file _iter_1.caffemodel
I1018 03:59:57.650413  1063 sgd_solver.cpp:273] Snapshotting solver state to binary proto file _iter_1.solverstate
I1018 03:59:57.673399  1063 solver.cpp:331] Optimization Done.
I1018 03:59:57.673434  1063 caffe.cpp:320] Optimization Done.
