I0404 20:55:50.146903 26096 caffe.cpp:218] Using GPUs 0
I0404 20:55:50.302062 26096 caffe.cpp:223] GPU 0: GeForce GTX 1080
I0404 20:55:51.412291 26096 solver.cpp:44] Initializing solver from parameters: 
test_iter: 1
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 1
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: "./"
solver_mode: GPU
device_id: 0
net: "out.prototxt"
train_state {
  level: 0
  stage: ""
}
I0404 20:55:51.412456 26096 solver.cpp:87] Creating training net from net file: out.prototxt
I0404 20:55:51.423825 26096 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer x_test
I0404 20:55:51.423840 26096 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer y_test
I0404 20:55:51.423851 26096 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0404 20:55:51.423903 26096 net.cpp:53] Initializing net from parameters: 
name: "DummyNetbyHand"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "x_train"
  type: "DummyData"
  top: "data"
  include {
    phase: TRAIN
  }
  dummy_data_param {
    data_filler {
      type: "constant"
    }
    shape {
      dim: 124
      dim: 3
      dim: 215
      dim: 215
    }
  }
}
layer {
  name: "y_train"
  type: "DummyData"
  top: "label"
  include {
    phase: TRAIN
  }
  dummy_data_param {
    data_filler {
      type: "constant"
    }
    shape {
      dim: 124
    }
  }
}
layer {
  name: "CONV"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 24
    kernel_size: 4
    stride: 1
    weight_filler {
      type: "gaussian"
    }
    bias_filler {
      type: "gaussian"
    }
  }
}
layer {
  name: "POOLING"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "IP"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 1
  }
  inner_product_param {
    num_output: 3017
    weight_filler {
      type: "gaussian"
    }
    bias_filler {
      type: "gaussian"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0404 20:55:51.424147 26096 layer_factory.hpp:77] Creating layer x_train
I0404 20:55:51.424185 26096 net.cpp:86] Creating Layer x_train
I0404 20:55:51.424199 26096 net.cpp:382] x_train -> data
I0404 20:55:51.470314 26096 net.cpp:124] Setting up x_train
I0404 20:55:51.470394 26096 net.cpp:131] Top shape: 124 3 215 215 (17195700)
I0404 20:55:51.470401 26096 net.cpp:139] Memory required for data: 68782800
I0404 20:55:51.470422 26096 layer_factory.hpp:77] Creating layer y_train
I0404 20:55:51.470475 26096 net.cpp:86] Creating Layer y_train
I0404 20:55:51.470489 26096 net.cpp:382] y_train -> label
I0404 20:55:51.470588 26096 net.cpp:124] Setting up y_train
I0404 20:55:51.470599 26096 net.cpp:131] Top shape: 124 (124)
I0404 20:55:51.470603 26096 net.cpp:139] Memory required for data: 68783296
I0404 20:55:51.470607 26096 layer_factory.hpp:77] Creating layer CONV
I0404 20:55:51.470638 26096 net.cpp:86] Creating Layer CONV
I0404 20:55:51.470650 26096 net.cpp:408] CONV <- data
I0404 20:55:51.470679 26096 net.cpp:382] CONV -> conv1
I0404 20:55:51.471861 26096 cudnn_conv_layer.cpp:21] start cudnn_conv LayerSetUp()
I0404 20:55:51.972481 26096 cudnn_conv_layer.cpp:89] End cudnn_conv LayerSetUp()
I0404 20:55:51.973177 26096 cudnn_conv_layer.cpp:96] Start cudnn_conv reshape()
I0404 20:55:51.978863 26096 cudnn_conv_layer.cpp:196]  [CONV] reallocate 838596
I0404 20:55:51.979039 26096 cudnn_conv_layer.cpp:234] End cudnn_conv reshape()
I0404 20:55:51.979049 26096 net.cpp:124] Setting up CONV
I0404 20:55:51.979063 26096 net.cpp:131] Top shape: 124 24 212 212 (133753344)
I0404 20:55:51.979066 26096 net.cpp:139] Memory required for data: 603796672
I0404 20:55:51.979125 26096 layer_factory.hpp:77] Creating layer POOLING
I0404 20:55:51.979156 26096 net.cpp:86] Creating Layer POOLING
I0404 20:55:51.979171 26096 net.cpp:408] POOLING <- conv1
I0404 20:55:51.979188 26096 net.cpp:382] POOLING -> pool1
I0404 20:55:51.979252 26096 net.cpp:124] Setting up POOLING
I0404 20:55:51.979266 26096 net.cpp:131] Top shape: 124 24 106 106 (33438336)
I0404 20:55:51.979270 26096 net.cpp:139] Memory required for data: 737550016
I0404 20:55:51.979276 26096 layer_factory.hpp:77] Creating layer IP
I0404 20:55:51.979296 26096 net.cpp:86] Creating Layer IP
I0404 20:55:51.979302 26096 net.cpp:408] IP <- pool1
I0404 20:55:51.979315 26096 net.cpp:382] IP -> ip2
I0404 20:55:51.979333 26096 inner_product_layer.cpp:21] Start IP LayerSetUp
I0404 20:56:48.731714 26096 inner_product_layer.cpp:56] End IP LayerSetUp
I0404 20:56:48.731757 26096 inner_product_layer.cpp:69] Start IP Reshape()
I0404 20:56:48.731803 26096 inner_product_layer.cpp:84] End IP Reshape()
I0404 20:56:48.731811 26096 net.cpp:124] Setting up IP
I0404 20:56:48.731823 26096 net.cpp:131] Top shape: 124 3017 (374108)
I0404 20:56:48.731827 26096 net.cpp:139] Memory required for data: 739046448
I0404 20:56:48.731856 26096 layer_factory.hpp:77] Creating layer loss
I0404 20:56:48.731881 26096 net.cpp:86] Creating Layer loss
I0404 20:56:48.731889 26096 net.cpp:408] loss <- ip2
I0404 20:56:48.731901 26096 net.cpp:408] loss <- label
I0404 20:56:48.731914 26096 net.cpp:382] loss -> loss
I0404 20:56:48.731935 26096 layer_factory.hpp:77] Creating layer loss
I0404 20:56:48.738003 26096 net.cpp:124] Setting up loss
I0404 20:56:48.738020 26096 net.cpp:131] Top shape: (1)
I0404 20:56:48.738025 26096 net.cpp:134]     with loss weight 1
I0404 20:56:48.747337 26096 net.cpp:139] Memory required for data: 739046452
I0404 20:56:48.747350 26096 net.cpp:200] loss needs backward computation.
I0404 20:56:48.747360 26096 net.cpp:200] IP needs backward computation.
I0404 20:56:48.747366 26096 net.cpp:200] POOLING needs backward computation.
I0404 20:56:48.747372 26096 net.cpp:200] CONV needs backward computation.
I0404 20:56:48.747380 26096 net.cpp:202] y_train does not need backward computation.
I0404 20:56:48.747385 26096 net.cpp:202] x_train does not need backward computation.
I0404 20:56:48.747392 26096 net.cpp:244] This network produces output loss
I0404 20:56:48.747413 26096 net.cpp:257] Network initialization done.
I0404 20:56:48.747699 26096 solver.cpp:173] Creating test net (#0) specified by net file: out.prototxt
I0404 20:56:48.747742 26096 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer x_train
I0404 20:56:48.747750 26096 net.cpp:296] The NetState phase (1) differed from the phase (0) specified by a rule in layer y_train
I0404 20:56:48.747814 26096 net.cpp:53] Initializing net from parameters: 
name: "DummyNetbyHand"
state {
  phase: TEST
}
layer {
  name: "x_test"
  type: "DummyData"
  top: "data"
  include {
    phase: TEST
  }
  dummy_data_param {
    data_filler {
      type: "constant"
    }
    shape {
      dim: 124
      dim: 3
      dim: 215
      dim: 215
    }
  }
}
layer {
  name: "y_test"
  type: "DummyData"
  top: "label"
  include {
    phase: TEST
  }
  dummy_data_param {
    data_filler {
      type: "constant"
    }
    shape {
      dim: 124
    }
  }
}
layer {
  name: "CONV"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 24
    kernel_size: 4
    stride: 1
    weight_filler {
      type: "gaussian"
    }
    bias_filler {
      type: "gaussian"
    }
  }
}
layer {
  name: "POOLING"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "IP"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 1
  }
  inner_product_param {
    num_output: 3017
    weight_filler {
      type: "gaussian"
    }
    bias_filler {
      type: "gaussian"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0404 20:56:48.747934 26096 layer_factory.hpp:77] Creating layer x_test
I0404 20:56:48.747953 26096 net.cpp:86] Creating Layer x_test
I0404 20:56:48.747962 26096 net.cpp:382] x_test -> data
I0404 20:56:48.791901 26096 net.cpp:124] Setting up x_test
I0404 20:56:48.791932 26096 net.cpp:131] Top shape: 124 3 215 215 (17195700)
I0404 20:56:48.791937 26096 net.cpp:139] Memory required for data: 68782800
I0404 20:56:48.791955 26096 layer_factory.hpp:77] Creating layer y_test
I0404 20:56:48.791996 26096 net.cpp:86] Creating Layer y_test
I0404 20:56:48.792006 26096 net.cpp:382] y_test -> label
I0404 20:56:48.792106 26096 net.cpp:124] Setting up y_test
I0404 20:56:48.792127 26096 net.cpp:131] Top shape: 124 (124)
I0404 20:56:48.792131 26096 net.cpp:139] Memory required for data: 68783296
I0404 20:56:48.792136 26096 layer_factory.hpp:77] Creating layer label_y_test_0_split
I0404 20:56:48.792152 26096 net.cpp:86] Creating Layer label_y_test_0_split
I0404 20:56:48.792158 26096 net.cpp:408] label_y_test_0_split <- label
I0404 20:56:48.792171 26096 net.cpp:382] label_y_test_0_split -> label_y_test_0_split_0
I0404 20:56:48.792186 26096 net.cpp:382] label_y_test_0_split -> label_y_test_0_split_1
I0404 20:56:48.792227 26096 net.cpp:124] Setting up label_y_test_0_split
I0404 20:56:48.792235 26096 net.cpp:131] Top shape: 124 (124)
I0404 20:56:48.792240 26096 net.cpp:131] Top shape: 124 (124)
I0404 20:56:48.792243 26096 net.cpp:139] Memory required for data: 68784288
I0404 20:56:48.792248 26096 layer_factory.hpp:77] Creating layer CONV
I0404 20:56:48.792265 26096 net.cpp:86] Creating Layer CONV
I0404 20:56:48.792273 26096 net.cpp:408] CONV <- data
I0404 20:56:48.792285 26096 net.cpp:382] CONV -> conv1
I0404 20:56:48.792521 26096 cudnn_conv_layer.cpp:21] start cudnn_conv LayerSetUp()
I0404 20:56:48.793133 26096 cudnn_conv_layer.cpp:89] End cudnn_conv LayerSetUp()
I0404 20:56:48.793301 26096 cudnn_conv_layer.cpp:96] Start cudnn_conv reshape()
I0404 20:56:48.793318 26096 cudnn_conv_layer.cpp:196]  [CONV] reallocate 838596
I0404 20:56:48.793335 26096 cudnn_conv_layer.cpp:234] End cudnn_conv reshape()
I0404 20:56:48.793339 26096 net.cpp:124] Setting up CONV
I0404 20:56:48.793346 26096 net.cpp:131] Top shape: 124 24 212 212 (133753344)
I0404 20:56:48.793350 26096 net.cpp:139] Memory required for data: 603797664
I0404 20:56:48.793372 26096 layer_factory.hpp:77] Creating layer POOLING
I0404 20:56:48.793390 26096 net.cpp:86] Creating Layer POOLING
I0404 20:56:48.793396 26096 net.cpp:408] POOLING <- conv1
I0404 20:56:48.793408 26096 net.cpp:382] POOLING -> pool1
I0404 20:56:48.793452 26096 net.cpp:124] Setting up POOLING
I0404 20:56:48.793462 26096 net.cpp:131] Top shape: 124 24 106 106 (33438336)
I0404 20:56:48.793465 26096 net.cpp:139] Memory required for data: 737551008
I0404 20:56:48.793470 26096 layer_factory.hpp:77] Creating layer IP
I0404 20:56:48.793483 26096 net.cpp:86] Creating Layer IP
I0404 20:56:48.793489 26096 net.cpp:408] IP <- pool1
I0404 20:56:48.793503 26096 net.cpp:382] IP -> ip2
I0404 20:56:48.793515 26096 inner_product_layer.cpp:21] Start IP LayerSetUp
I0404 20:57:45.520870 26096 inner_product_layer.cpp:56] End IP LayerSetUp
I0404 20:57:45.520951 26096 inner_product_layer.cpp:69] Start IP Reshape()
I0404 20:57:45.521018 26096 inner_product_layer.cpp:84] End IP Reshape()
I0404 20:57:45.521025 26096 net.cpp:124] Setting up IP
I0404 20:57:45.521039 26096 net.cpp:131] Top shape: 124 3017 (374108)
I0404 20:57:45.521042 26096 net.cpp:139] Memory required for data: 739047440
I0404 20:57:45.521073 26096 layer_factory.hpp:77] Creating layer ip2_IP_0_split
I0404 20:57:45.521095 26096 net.cpp:86] Creating Layer ip2_IP_0_split
I0404 20:57:45.521102 26096 net.cpp:408] ip2_IP_0_split <- ip2
I0404 20:57:45.521117 26096 net.cpp:382] ip2_IP_0_split -> ip2_IP_0_split_0
I0404 20:57:45.521133 26096 net.cpp:382] ip2_IP_0_split -> ip2_IP_0_split_1
I0404 20:57:45.521173 26096 net.cpp:124] Setting up ip2_IP_0_split
I0404 20:57:45.521188 26096 net.cpp:131] Top shape: 124 3017 (374108)
I0404 20:57:45.521193 26096 net.cpp:131] Top shape: 124 3017 (374108)
I0404 20:57:45.521196 26096 net.cpp:139] Memory required for data: 742040304
I0404 20:57:45.521201 26096 layer_factory.hpp:77] Creating layer accuracy
I0404 20:57:45.521222 26096 net.cpp:86] Creating Layer accuracy
I0404 20:57:45.521229 26096 net.cpp:408] accuracy <- ip2_IP_0_split_0
I0404 20:57:45.521237 26096 net.cpp:408] accuracy <- label_y_test_0_split_0
I0404 20:57:45.521246 26096 net.cpp:382] accuracy -> accuracy
I0404 20:57:45.521270 26096 net.cpp:124] Setting up accuracy
I0404 20:57:45.521276 26096 net.cpp:131] Top shape: (1)
I0404 20:57:45.521280 26096 net.cpp:139] Memory required for data: 742040308
I0404 20:57:45.521284 26096 layer_factory.hpp:77] Creating layer loss
I0404 20:57:45.521296 26096 net.cpp:86] Creating Layer loss
I0404 20:57:45.521302 26096 net.cpp:408] loss <- ip2_IP_0_split_1
I0404 20:57:45.521311 26096 net.cpp:408] loss <- label_y_test_0_split_1
I0404 20:57:45.521319 26096 net.cpp:382] loss -> loss
I0404 20:57:45.521332 26096 layer_factory.hpp:77] Creating layer loss
I0404 20:57:45.529897 26096 net.cpp:124] Setting up loss
I0404 20:57:45.529914 26096 net.cpp:131] Top shape: (1)
I0404 20:57:45.529918 26096 net.cpp:134]     with loss weight 1
I0404 20:57:45.529929 26096 net.cpp:139] Memory required for data: 742040312
I0404 20:57:45.529937 26096 net.cpp:200] loss needs backward computation.
I0404 20:57:45.529943 26096 net.cpp:202] accuracy does not need backward computation.
I0404 20:57:45.529948 26096 net.cpp:200] ip2_IP_0_split needs backward computation.
I0404 20:57:45.529953 26096 net.cpp:200] IP needs backward computation.
I0404 20:57:45.529958 26096 net.cpp:200] POOLING needs backward computation.
I0404 20:57:45.529961 26096 net.cpp:200] CONV needs backward computation.
I0404 20:57:45.529966 26096 net.cpp:202] label_y_test_0_split does not need backward computation.
I0404 20:57:45.529973 26096 net.cpp:202] y_test does not need backward computation.
I0404 20:57:45.529976 26096 net.cpp:202] x_test does not need backward computation.
I0404 20:57:45.529980 26096 net.cpp:244] This network produces output accuracy
I0404 20:57:45.529988 26096 net.cpp:244] This network produces output loss
I0404 20:57:45.530002 26096 net.cpp:257] Network initialization done.
I0404 20:57:45.530058 26096 solver.cpp:56] Solver scaffolding done.
I0404 20:57:45.530225 26096 caffe.cpp:248] Starting Optimization
I0404 20:57:45.530231 26096 solver.cpp:273] Solving DummyNetbyHand
I0404 20:57:45.530234 26096 solver.cpp:274] Learning Rate Policy: inv
I0404 20:57:45.530521 26096 syncedmem.cpp:79] to_gpu(),UNINIT : allocate and memset  4608
I0404 20:57:45.530556 26096 syncedmem.cpp:79] to_gpu(),UNINIT : allocate and memset  96
I0404 20:57:45.531770 26096 syncedmem.cpp:79] to_gpu(),UNINIT : allocate and memset  3254305152
I0404 20:57:45.531797 26096 syncedmem.cpp:79] to_gpu(),UNINIT : allocate and memset  12068
I0404 20:57:45.531810 26096 solver.cpp:331] Iteration 0, Testing net (#0)
I0404 20:57:45.531821 26096 net.cpp:687] Ignoring source layer x_train
I0404 20:57:45.531824 26096 net.cpp:687] Ignoring source layer y_train
I0404 20:57:45.531827 26096 net.cpp:690] Copying source layer CONV
I0404 20:57:45.555034 26096 net.cpp:690] Copying source layer POOLING
I0404 20:57:45.555050 26096 net.cpp:690] Copying source layer IP
I0404 20:57:45.816153 26096 net.cpp:690] Copying source layer loss
I0404 20:57:45.829468 26096 net.cpp:596]  [Forward] [x_test] top blob data data size: 68782800
I0404 20:57:45.829483 26096 net.cpp:596]  [Forward] [y_test] top blob label data size: 496
I0404 20:57:45.829499 26096 net.cpp:596]  [Forward] [label_y_test_0_split] top blob label_y_test_0_split_0 data size: 496
I0404 20:57:45.829504 26096 net.cpp:596]  [Forward] [label_y_test_0_split] top blob label_y_test_0_split_1 data size: 496
I0404 20:57:45.829607 26096 cudnn_conv_layer.cpp:96] Start cudnn_conv reshape()
I0404 20:57:45.829630 26096 cudnn_conv_layer.cpp:234] End cudnn_conv reshape()
I0404 20:57:45.829694 26096 syncedmem.cpp:88] to_gpu(),HEAD_AT_CPU : allocate and copy from CPU to GPU  4608
I0404 20:57:45.835275 26096 syncedmem.cpp:88] to_gpu(),HEAD_AT_CPU : allocate and copy from CPU to GPU  68782800
I0404 20:57:45.835728 26096 syncedmem.cpp:79] to_gpu(),UNINIT : allocate and memset  535013376
I0404 20:57:45.835737 26096 cudnn_conv_layer.cu:17] Start cudnn_conv Forward_gpu()
I0404 20:58:21.205888 26096 syncedmem.cpp:88] to_gpu(),HEAD_AT_CPU : allocate and copy from CPU to GPU  96
I0404 20:58:21.226670 26096 cudnn_conv_layer.cu:46] End cudnn_conv Forward_gpu()
I0404 20:58:21.226693 26096 net.cpp:596]  [Forward] [CONV] top blob conv1 data size: 535013376
I0404 20:58:21.226707 26096 net.cpp:610]  [Forward]  [CONV] param blob 0 data size: 4608
I0404 20:58:21.226713 26096 net.cpp:610]  [Forward]  [CONV] param blob 1 data size: 96
I0404 20:58:21.226748 26096 syncedmem.cpp:92] to_gpu(),HEAD_AT_GPU : referencing data on GPU  535013376
I0404 20:58:21.240578 26096 syncedmem.cpp:79] to_gpu(),UNINIT : allocate and memset  133753344
I0404 20:58:21.240984 26096 syncedmem.cpp:79] to_gpu(),UNINIT : allocate and memset  133753344
I0404 20:58:21.241156 26096 net.cpp:596]  [Forward] [POOLING] top blob pool1 data size: 133753344
I0404 20:58:21.241170 26096 inner_product_layer.cpp:69] Start IP Reshape()
I0404 20:58:21.241191 26096 inner_product_layer.cpp:84] End IP Reshape()
I0404 20:58:21.241204 26096 syncedmem.cpp:92] to_gpu(),HEAD_AT_GPU : referencing data on GPU  133753344
I0404 20:58:21.241235 26096 syncedmem.cpp:79] to_gpu(),UNINIT : allocate and memset  1496432
I0404 20:58:21.544332 26096 syncedmem.cpp:88] to_gpu(),HEAD_AT_CPU : allocate and copy from CPU to GPU  3254305152
I0404 20:58:21.544355 26096 inner_product_layer.cu:15] Started IP Forward_gpu()
I0404 20:58:21.686648 26096 syncedmem.cpp:88] to_gpu(),HEAD_AT_CPU : allocate and copy from CPU to GPU  12068
I0404 20:58:21.686731 26096 syncedmem.cpp:88] to_gpu(),HEAD_AT_CPU : allocate and copy from CPU to GPU  496
I0404 20:58:21.686929 26096 inner_product_layer.cu:32] End IP Forward_gpu()
I0404 20:58:21.686942 26096 net.cpp:596]  [Forward] [IP] top blob ip2 data size: 1496432
I0404 20:58:21.686951 26096 net.cpp:610]  [Forward]  [IP] param blob 0 data size: 3254305152
I0404 20:58:21.686957 26096 net.cpp:610]  [Forward]  [IP] param blob 1 data size: 12068
I0404 20:58:21.686978 26096 net.cpp:596]  [Forward] [ip2_IP_0_split] top blob ip2_IP_0_split_0 data size: 1496432
I0404 20:58:21.686983 26096 net.cpp:596]  [Forward] [ip2_IP_0_split] top blob ip2_IP_0_split_1 data size: 1496432
I0404 20:58:21.722571 26096 net.cpp:596]  [Forward] [accuracy] top blob accuracy data size: 4
I0404 20:58:21.722623 26096 softmax_loss_layer.cu:34] start SoftmaxWithLoss Forward_gpu()
I0404 20:58:21.723089 26096 syncedmem.cpp:79] to_gpu(),UNINIT : allocate and memset  1496432
I0404 20:58:21.739817 26096 syncedmem.cpp:92] to_gpu(),HEAD_AT_GPU : referencing data on GPU  1496432
I0404 20:58:21.740398 26096 syncedmem.cpp:88] to_gpu(),HEAD_AT_CPU : allocate and copy from CPU to GPU  1496432
I0404 20:58:21.748222 26096 syncedmem.cpp:92] to_gpu(),HEAD_AT_GPU : referencing data on GPU  1496432
I0404 20:58:21.748276 26096 syncedmem.cpp:88] to_gpu(),HEAD_AT_CPU : allocate and copy from CPU to GPU  496
I0404 20:58:21.748670 26096 syncedmem.cpp:79] to_gpu(),UNINIT : allocate and memset  1496432
I0404 20:58:21.748997 26096 softmax_loss_layer.cu:65] end SoftmaxWithLoss Forward_gpu()
I0404 20:58:21.749034 26096 syncedmem.cpp:88] to_gpu(),HEAD_AT_CPU : allocate and copy from CPU to GPU  4
I0404 20:58:21.749065 26096 syncedmem.cpp:88] to_gpu(),HEAD_AT_CPU : allocate and copy from CPU to GPU  4
I0404 20:58:21.749263 26096 net.cpp:596]  [Forward] [loss] top blob loss data size: 4
I0404 20:58:21.749296 26096 solver.cpp:398]     Test net output #0: accuracy = 0
I0404 20:58:21.749318 26096 solver.cpp:398]     Test net output #1: loss = 87.3366 (* 1 = 87.3366 loss)
I0404 20:58:21.749337 26096 net.cpp:596]  [Forward] [x_train] top blob data data size: 68782800
I0404 20:58:21.749343 26096 net.cpp:596]  [Forward] [y_train] top blob label data size: 496
I0404 20:58:21.749467 26096 cudnn_conv_layer.cpp:96] Start cudnn_conv reshape()
I0404 20:58:21.749497 26096 cudnn_conv_layer.cpp:234] End cudnn_conv reshape()
I0404 20:58:21.755048 26096 syncedmem.cpp:88] to_gpu(),HEAD_AT_CPU : allocate and copy from CPU to GPU  68782800
I0404 20:58:21.755635 26096 syncedmem.cpp:79] to_gpu(),UNINIT : allocate and memset  535013376
I0404 20:58:21.755645 26096 cudnn_conv_layer.cu:17] Start cudnn_conv Forward_gpu()
