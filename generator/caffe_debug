I0412 16:54:00.813343 27914 caffe.cpp:352] Use CPU.
I0412 16:54:01.441545 27914 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer x_test
I0412 16:54:01.441598 27914 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer y_test
I0412 16:54:01.441622 27914 net.cpp:296] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0412 16:54:01.441727 27914 net.cpp:53] Initializing net from parameters: 
name: "DummyNetbyHand"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "x_train"
  type: "DummyData"
  top: "data"
  include {
    phase: TRAIN
  }
  dummy_data_param {
    data_filler {
      type: "constant"
    }
    shape {
      dim: 30
      dim: 3
      dim: 61
      dim: 61
    }
  }
}
layer {
  name: "y_train"
  type: "DummyData"
  top: "label"
  include {
    phase: TRAIN
  }
  dummy_data_param {
    data_filler {
      type: "constant"
    }
    shape {
      dim: 30
    }
  }
}
layer {
  name: "CONV"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 1
  }
  convolution_param {
    num_output: 73
    kernel_size: 10
    stride: 1
    weight_filler {
      type: "gaussian"
    }
    bias_filler {
      type: "gaussian"
    }
  }
}
layer {
  name: "POOLING"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "IP"
  type: "InnerProduct"
  bottom: "pool1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 1
  }
  inner_product_param {
    num_output: 2326
    weight_filler {
      type: "gaussian"
    }
    bias_filler {
      type: "gaussian"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0412 16:54:01.442225 27914 layer_factory.hpp:77] Creating layer x_train
I0412 16:54:01.442301 27914 net.cpp:86] Creating Layer x_train
I0412 16:54:01.442327 27914 net.cpp:382] x_train -> data
I0412 16:54:01.447862 27914 net.cpp:124] Setting up x_train
I0412 16:54:01.447911 27914 net.cpp:131] Top shape: 30 3 61 61 (334890)
I0412 16:54:01.447917 27914 net.cpp:139] Memory required for data: 1339560
I0412 16:54:01.447933 27914 layer_factory.hpp:77] Creating layer y_train
I0412 16:54:01.447963 27914 net.cpp:86] Creating Layer y_train
I0412 16:54:01.447981 27914 net.cpp:382] y_train -> label
I0412 16:54:01.448037 27914 net.cpp:124] Setting up y_train
I0412 16:54:01.448055 27914 net.cpp:131] Top shape: 30 (30)
I0412 16:54:01.448060 27914 net.cpp:139] Memory required for data: 1339680
I0412 16:54:01.448068 27914 layer_factory.hpp:77] Creating layer CONV
I0412 16:54:01.448115 27914 net.cpp:86] Creating Layer CONV
I0412 16:54:01.448134 27914 net.cpp:408] CONV <- data
I0412 16:54:01.448177 27914 net.cpp:382] CONV -> conv1
I0412 16:54:01.451333 27914 cudnn_conv_layer.cpp:21] start cudnn_conv LayerSetUp()
I0412 16:54:01.816654 27914 cudnn_conv_layer.cpp:89] End cudnn_conv LayerSetUp()
I0412 16:54:01.816774 27914 cudnn_conv_layer.cpp:96] Start cudnn_conv reshape()
I0412 16:54:01.816844 27914 cudnn_conv_layer.cpp:196]  [CONV] reallocate 101064
I0412 16:54:01.817157 27914 cudnn_conv_layer.cpp:234] End cudnn_conv reshape()
I0412 16:54:01.817175 27914 net.cpp:124] Setting up CONV
I0412 16:54:01.817199 27914 net.cpp:131] Top shape: 30 73 52 52 (5921760)
I0412 16:54:01.817206 27914 net.cpp:139] Memory required for data: 25026720
I0412 16:54:01.817312 27914 layer_factory.hpp:77] Creating layer POOLING
I0412 16:54:01.817364 27914 net.cpp:86] Creating Layer POOLING
I0412 16:54:01.817380 27914 net.cpp:408] POOLING <- conv1
I0412 16:54:01.817414 27914 net.cpp:382] POOLING -> pool1
I0412 16:54:01.817481 27914 net.cpp:124] Setting up POOLING
I0412 16:54:01.817498 27914 net.cpp:131] Top shape: 30 73 26 26 (1480440)
I0412 16:54:01.817504 27914 net.cpp:139] Memory required for data: 30948480
I0412 16:54:01.817513 27914 layer_factory.hpp:77] Creating layer IP
I0412 16:54:01.817551 27914 net.cpp:86] Creating Layer IP
I0412 16:54:01.817562 27914 net.cpp:408] IP <- pool1
I0412 16:54:01.817596 27914 net.cpp:382] IP -> ip2
I0412 16:54:01.817643 27914 inner_product_layer.cpp:21] Start IP LayerSetUp
I0412 16:54:11.448330 27914 inner_product_layer.cpp:56] End IP LayerSetUp
I0412 16:54:11.448377 27914 inner_product_layer.cpp:69] Start IP Reshape()
I0412 16:54:11.448397 27914 inner_product_layer.cpp:84] End IP Reshape()
I0412 16:54:11.448412 27914 net.cpp:124] Setting up IP
I0412 16:54:11.448433 27914 net.cpp:131] Top shape: 30 2326 (69780)
I0412 16:54:11.448439 27914 net.cpp:139] Memory required for data: 31227600
I0412 16:54:11.448488 27914 layer_factory.hpp:77] Creating layer loss
I0412 16:54:11.448540 27914 net.cpp:86] Creating Layer loss
I0412 16:54:11.448555 27914 net.cpp:408] loss <- ip2
I0412 16:54:11.448572 27914 net.cpp:408] loss <- label
I0412 16:54:11.448590 27914 net.cpp:382] loss -> loss
I0412 16:54:11.448626 27914 layer_factory.hpp:77] Creating layer loss
I0412 16:54:11.449399 27914 net.cpp:124] Setting up loss
I0412 16:54:11.449414 27914 net.cpp:131] Top shape: (1)
I0412 16:54:11.449419 27914 net.cpp:134]     with loss weight 1
I0412 16:54:11.449443 27914 net.cpp:139] Memory required for data: 31227604
I0412 16:54:11.449453 27914 net.cpp:200] loss needs backward computation.
I0412 16:54:11.449461 27914 net.cpp:200] IP needs backward computation.
I0412 16:54:11.449467 27914 net.cpp:200] POOLING needs backward computation.
I0412 16:54:11.449475 27914 net.cpp:200] CONV needs backward computation.
I0412 16:54:11.449481 27914 net.cpp:202] y_train does not need backward computation.
I0412 16:54:11.449486 27914 net.cpp:202] x_train does not need backward computation.
I0412 16:54:11.449493 27914 net.cpp:244] This network produces output loss
I0412 16:54:11.449512 27914 net.cpp:257] Network initialization done.
I0412 16:54:11.449589 27914 caffe.cpp:360] Performing Forward
I0412 16:54:11.449601 27914 net.cpp:596]  [Forward] [x_train] top blob data data size: 1339560
I0412 16:54:11.449607 27914 net.cpp:596]  [Forward] [y_train] top blob label data size: 120
I0412 16:54:11.449637 27914 cudnn_conv_layer.cpp:96] Start cudnn_conv reshape()
I0412 16:54:11.449664 27914 cudnn_conv_layer.cpp:234] End cudnn_conv reshape()
I0412 16:54:11.759467 27914 net.cpp:596]  [Forward] [CONV] top blob conv1 data size: 23687040
I0412 16:54:11.759501 27914 net.cpp:610]  [Forward]  [CONV] param blob 0 data size: 87600
I0412 16:54:11.759506 27914 net.cpp:610]  [Forward]  [CONV] param blob 1 data size: 292
I0412 16:54:11.824592 27914 net.cpp:596]  [Forward] [POOLING] top blob pool1 data size: 5921760
I0412 16:54:11.824633 27914 inner_product_layer.cpp:69] Start IP Reshape()
I0412 16:54:11.824659 27914 inner_product_layer.cpp:84] End IP Reshape()
I0412 16:54:12.287401 27914 net.cpp:596]  [Forward] [IP] top blob ip2 data size: 279120
I0412 16:54:12.287442 27914 net.cpp:610]  [Forward]  [IP] param blob 0 data size: 459133792
I0412 16:54:12.287446 27914 net.cpp:610]  [Forward]  [IP] param blob 1 data size: 9304
I0412 16:54:12.292094 27914 net.cpp:596]  [Forward] [loss] top blob loss data size: 4
I0412 16:54:12.292104 27914 caffe.cpp:365] Initial loss: 87.3365
I0412 16:54:12.292121 27914 caffe.cpp:366] Performing Backward
I0412 16:54:12.292304 27914 net.cpp:628]  [Backward] [loss] bottom blob ip2 diff size: 279120
I0412 16:54:14.196339 27914 net.cpp:628]  [Backward] [IP] bottom blob pool1 diff size: 5921760
I0412 16:54:14.196390 27914 net.cpp:641]  [Backward] [IP] param blob 0 diff size: 459133792
I0412 16:54:14.196396 27914 net.cpp:641]  [Backward] [IP] param blob 1 diff size: 9304
I0412 16:54:14.223248 27914 net.cpp:628]  [Backward] [POOLING] bottom blob conv1 diff size: 23687040
I0412 16:54:14.534688 27914 net.cpp:641]  [Backward] [CONV] param blob 0 diff size: 87600
I0412 16:54:14.534724 27914 net.cpp:641]  [Backward] [CONV] param blob 1 diff size: 292
I0412 16:54:14.534730 27914 caffe.cpp:374] *** Benchmark begins ***
I0412 16:54:14.534734 27914 caffe.cpp:375] Testing for 1 iterations.
I0412 16:54:14.534915 27914 cudnn_conv_layer.cpp:96] Start cudnn_conv reshape()
I0412 16:54:14.534948 27914 cudnn_conv_layer.cpp:234] End cudnn_conv reshape()
I0412 16:54:14.897596 27914 inner_product_layer.cpp:69] Start IP Reshape()
I0412 16:54:14.897640 27914 inner_product_layer.cpp:84] End IP Reshape()
I0412 16:54:17.486970 27914 caffe.cpp:403] Iteration: 1 forward-backward time: 2952 ms.
I0412 16:54:17.487021 27914 caffe.cpp:406] Average time per layer: 
I0412 16:54:17.487025 27914 caffe.cpp:409]  [time]    x_train	forward: 0.004 ms.
I0412 16:54:17.487040 27914 caffe.cpp:412]    x_train	backward: 0.001 ms.
I0412 16:54:17.487045 27914 caffe.cpp:409]  [time]    y_train	forward: 0.002 ms.
I0412 16:54:17.487048 27914 caffe.cpp:412]    y_train	backward: 0.002 ms.
I0412 16:54:17.487053 27914 caffe.cpp:409]  [time]       CONV	forward: 300.527 ms.
I0412 16:54:17.487058 27914 caffe.cpp:412]       CONV	backward: 308.429 ms.
I0412 16:54:17.487062 27914 caffe.cpp:409]  [time]    POOLING	forward: 62.144 ms.
I0412 16:54:17.487067 27914 caffe.cpp:412]    POOLING	backward: 21.72 ms.
I0412 16:54:17.487071 27914 caffe.cpp:409]  [time]         IP	forward: 463.016 ms.
I0412 16:54:17.487076 27914 caffe.cpp:412]         IP	backward: 1791.38 ms.
I0412 16:54:17.487081 27914 caffe.cpp:409]  [time]       loss	forward: 4.647 ms.
I0412 16:54:17.487085 27914 caffe.cpp:412]       loss	backward: 0.065 ms.
I0412 16:54:17.487092 27914 caffe.cpp:417] Average Forward pass: 830.425 ms.
I0412 16:54:17.487097 27914 caffe.cpp:419] Average Backward pass: 2121.67 ms.
I0412 16:54:17.487102 27914 caffe.cpp:421] Average Forward-Backward: 2952 ms.
I0412 16:54:17.487107 27914 caffe.cpp:423] Total Time: 2952 ms.
I0412 16:54:17.487112 27914 caffe.cpp:424] *** Benchmark ends ***
I0412 16:54:17.487593 27914 cudnn_conv_layer.cpp:242] Start ~cudnn_conv() 
I0412 16:54:17.488206 27914 cudnn_conv_layer.cpp:259] during ~cudnn_conv() : Freed workspace size 101064
I0412 16:54:17.488219 27914 cudnn_conv_layer.cpp:268] End ~cudnn_conv()
